{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7c9e42c",
   "metadata": {},
   "source": [
    "# Separated.py 소개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84ef7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "from dora.log import fatal\n",
    "import torch as th\n",
    "import torchaudio as ta\n",
    "\n",
    "from .apply import apply_model, BagOfModels\n",
    "from .audio import AudioFile, convert_audio, save_audio\n",
    "from .pretrained import get_model_from_args, add_model_flags, ModelLoadingError\n",
    "\n",
    "\n",
    "def load_track(track, device, audio_channels, samplerate):\n",
    "    errors = {}\n",
    "    wav = None\n",
    "\n",
    "    try:\n",
    "        wav = AudioFile(track).read(\n",
    "            streams=0,\n",
    "            samplerate=samplerate,\n",
    "            channels=audio_channels).to(device)\n",
    "    except FileNotFoundError:\n",
    "        errors['ffmpeg'] = 'Ffmpeg is not installed.'\n",
    "    except subprocess.CalledProcessError:\n",
    "        errors['ffmpeg'] = 'FFmpeg could not read the file.'\n",
    "\n",
    "    if wav is None:\n",
    "        try:\n",
    "            wav, sr = ta.load(str(track))\n",
    "        except RuntimeError as err:\n",
    "            errors['torchaudio'] = err.args[0]\n",
    "        else:\n",
    "            wav = wav.to(device)\n",
    "            wav = convert_audio(wav, sr, samplerate, audio_channels)\n",
    "\n",
    "    if wav is None:\n",
    "        print(f\"Could not load file {track}. \"\n",
    "              \"Maybe it is not a supported file format? \")\n",
    "        for backend, error in errors.items():\n",
    "            print(f\"When trying to load using {backend}, got the following error: {error}\")\n",
    "        sys.exit(1)\n",
    "    return wav\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(\"demucs.separate\",\n",
    "                                     description=\"Separate the sources for the given tracks\")\n",
    "    parser.add_argument(\"tracks\", nargs='+', type=Path, default=[], help='Path to tracks')\n",
    "    add_model_flags(parser)\n",
    "    parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\")\n",
    "    parser.add_argument(\"-o\",\n",
    "                        \"--out\",\n",
    "                        type=Path,\n",
    "                        default=Path(\"separated\"),\n",
    "                        help=\"Folder where to put extracted tracks. A subfolder \"\n",
    "                        \"with the model name will be created.\")\n",
    "    parser.add_argument(\"-d\",\n",
    "                        \"--device\",\n",
    "                        default=\"cuda\" if th.cuda.is_available() else \"cpu\",\n",
    "                        help=\"Device to use, default is cuda if available else cpu\")\n",
    "    parser.add_argument(\"--shifts\",\n",
    "                        default=1,\n",
    "                        type=int,\n",
    "                        help=\"Number of random shifts for equivariant stabilization.\"\n",
    "                        \"Increase separation time but improves quality for Demucs. 10 was used \"\n",
    "                        \"in the original paper.\")\n",
    "    parser.add_argument(\"--overlap\",\n",
    "                        default=0.25,\n",
    "                        type=float,\n",
    "                        help=\"Overlap between the splits.\")\n",
    "    parser.add_argument(\"--no-split\",\n",
    "                        action=\"store_false\",\n",
    "                        dest=\"split\",\n",
    "                        default=True,\n",
    "                        help=\"Doesn't split audio in chunks. This can use large amounts of memory.\")\n",
    "    parser.add_argument(\"--mp3\", action=\"store_true\",\n",
    "                        help=\"Convert the output wavs to mp3.\")\n",
    "    parser.add_argument(\"--mp3-bitrate\",\n",
    "                        default=320,\n",
    "                        type=int,\n",
    "                        help=\"Bitrate of converted mp3.\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    try:\n",
    "        model = get_model_from_args(args)\n",
    "    except ModelLoadingError as error:\n",
    "        fatal(error.args[0])\n",
    "\n",
    "    if isinstance(model, BagOfModels):\n",
    "        print(f\"Selected model is a bag of {len(model.models)} models. \"\n",
    "              \"You will see that many progress bars per track.\")\n",
    "    model.to(args.device)\n",
    "    model.eval()\n",
    "\n",
    "    out = args.out / args.name\n",
    "    out.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Separated tracks will be stored in {out.resolve()}\")\n",
    "    for track in args.tracks:\n",
    "        if not track.exists():\n",
    "            print(\n",
    "                f\"File {track} does not exist. If the path contains spaces, \"\n",
    "                \"please try again after surrounding the entire path with quotes \\\"\\\".\",\n",
    "                file=sys.stderr)\n",
    "            continue\n",
    "        print(f\"Separating track {track}\")\n",
    "        wav = load_track(track, args.device, model.audio_channels, model.samplerate)\n",
    "\n",
    "        ref = wav.mean(0)\n",
    "        wav = (wav - ref.mean()) / ref.std()\n",
    "        sources = apply_model(model, wav[None], shifts=args.shifts, split=args.split,\n",
    "                              overlap=args.overlap, progress=True)[0]\n",
    "        sources = sources * ref.std() + ref.mean()\n",
    "\n",
    "        track_folder = out / track.name.rsplit(\".\", 1)[0]\n",
    "        track_folder.mkdir(exist_ok=True)\n",
    "        for source, name in zip(sources, model.sources):\n",
    "            source = source.cpu()\n",
    "            stem = str(track_folder / name)\n",
    "            if args.mp3:\n",
    "                stem += \".mp3\"\n",
    "            else:\n",
    "                stem += \".wav\"\n",
    "            save_audio(source, stem, model.samplerate)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35236a2b",
   "metadata": {},
   "source": [
    "# 해석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff68be36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # command line에 입력할 옵션들의 **집합 생성**\n",
    "    parser = argparse.ArgumentParser(\"demucs.separate\",\n",
    "                                     description=\"Separate the sources for the given tracks\")\n",
    "    # 옵션 집합 parser에 노래(track) 경로 옵션 추가\n",
    "    parser.add_argument(\"tracks\", nargs='+', type=Path, default=[], help='Path to tracks')\n",
    "    \n",
    "    # 아래 해석-1 셀 참조\n",
    "    # 추가적인 훈련한 mdx 양자화 모델을 default로 사용\n",
    "    add_model_flags(parser)\n",
    "    \n",
    "    # 옵션 추가 (주요 옵션만 설명)\n",
    "    # -d : cpu or cuda(=default)\n",
    "    # --mp3 : output을 mp3로 변환 (default는 wav)\n",
    "    parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\")\n",
    "    parser.add_argument(\"-o\",\n",
    "                        \"--out\",\n",
    "                        type=Path,\n",
    "                        default=Path(\"separated\"),\n",
    "                        help=\"Folder where to put extracted tracks. A subfolder \"\n",
    "                        \"with the model name will be created.\")\n",
    "    parser.add_argument(\"-d\",\n",
    "                        \"--device\",\n",
    "                        default=\"cuda\" if th.cuda.is_available() else \"cpu\",\n",
    "                        help=\"Device to use, default is cuda if available else cpu\")\n",
    "    parser.add_argument(\"--shifts\",\n",
    "                        default=1,\n",
    "                        type=int,\n",
    "                        help=\"Number of random shifts for equivariant stabilization.\"\n",
    "                        \"Increase separation time but improves quality for Demucs. 10 was used \"\n",
    "                        \"in the original paper.\")\n",
    "    parser.add_argument(\"--overlap\",\n",
    "                        default=0.25,\n",
    "                        type=float,\n",
    "                        help=\"Overlap between the splits.\")\n",
    "    parser.add_argument(\"--no-split\",\n",
    "                        action=\"store_false\",\n",
    "                        dest=\"split\",\n",
    "                        default=True,\n",
    "                        help=\"Doesn't split audio in chunks. This can use large amounts of memory.\")\n",
    "    parser.add_argument(\"--mp3\", action=\"store_true\",\n",
    "                        help=\"Convert the output wavs to mp3.\")\n",
    "    parser.add_argument(\"--mp3-bitrate\",\n",
    "                        default=320,\n",
    "                        type=int,\n",
    "                        help=\"Bitrate of converted mp3.\")\n",
    "    # 입력 받은 커맨드 옵션을 args 변수로 생성\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    try:\n",
    "        # 요약: AWS는 모델 4개 로드, 로컬은 정하기 나름\n",
    "        # 해석-2 셀 참조\n",
    "        model = get_model_from_args(args)\n",
    "    except ModelLoadingError as error:\n",
    "        fatal(error.args[0])\n",
    "    \n",
    "    # 모델 몇개 로드했는지 검사 및 프린트\n",
    "    if isinstance(model, BagOfModels):\n",
    "        print(f\"Selected model is a bag of {len(model.models)} models. \"\n",
    "              \"You will see that many progress bars per track.\")\n",
    "    # default는 cuda\n",
    "    model.to(args.device)\n",
    "    model.eval()\n",
    "    \n",
    "    # 결과물 저장 폴더 경로 / 모델 이름 \n",
    "    out = args.out / args.name\n",
    "    out.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Separated tracks will be stored in {out.resolve()}\")\n",
    "    \n",
    "    # 입력 받은 노래(track)가 1개 이상이어도 처리할 수 있게끔 for loop 사용\n",
    "    for track in args.tracks:\n",
    "        if not track.exists():\n",
    "            print(\n",
    "                f\"File {track} does not exist. If the path contains spaces, \"\n",
    "                \"please try again after surrounding the entire path with quotes \\\"\\\".\",\n",
    "                file=sys.stderr)\n",
    "            continue\n",
    "        print(f\"Separating track {track}\")\n",
    "        #### 여기부터 중요 ###########################################################\n",
    "        # AWS pretrained 모델의 경우 2채널, 44100sr 사용해서 wave 로드\n",
    "        # 로컬은 정하기 나름. wave 로드\n",
    "        # 해석-5 셀 참조\n",
    "        wav = load_track(track, args.device, model.audio_channels, model.samplerate)\n",
    "        \n",
    "        # wav 정규화\n",
    "        ref = wav.mean(0)\n",
    "        wav = (wav - ref.mean()) / ref.std()\n",
    "        \n",
    "        # 해석-6 셀 참조\n",
    "        # 요약 : 8초 단위로 wave 잘라서 model 실행\n",
    "        sources = apply_model(model, wav[None], shifts=args.shifts, split=args.split,\n",
    "                              overlap=args.overlap, progress=True)[0]\n",
    "        # output 정규화 해제(원 데이터와 같은 scale로 복구)\n",
    "        sources = sources * ref.std() + ref.mean()\n",
    "        \n",
    "        # 저장할 경로 설정\n",
    "        track_folder = out / track.name.rsplit(\".\", 1)[0]\n",
    "        track_folder.mkdir(exist_ok=True)\n",
    "        for source, name in zip(sources, model.sources):\n",
    "            source = source.cpu()\n",
    "            stem = str(track_folder / name)\n",
    "            if args.mp3:\n",
    "                stem += \".mp3\"\n",
    "            else:\n",
    "                stem += \".wav\"\n",
    "            save_audio(source, stem, model.samplerate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fd011e",
   "metadata": {},
   "source": [
    "## 해석-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230a958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_model_flags(parser):\n",
    "    # 아래 입력한 옵션(로컬 모델 or AWS pretrained 모델) 중 하나만 쓸 수 있음. 두 개 입력시 error 발생.\n",
    "    # 아무것도 입력하지 않으면, default는 -n의 mdx_extra_q 모델임\n",
    "    # https://docs.python.org/ko/3/library/argparse.html\n",
    "    group = parser.add_mutually_exclusive_group(required=False)\n",
    "    group.add_argument(\"-s\", \"--sig\", help=\"Locally trained XP signature.\")\n",
    "    group.add_argument(\"-n\", \"--name\", default=\"mdx_extra_q\",\n",
    "                       help=\"Pretrained model name or signature. Default is mdx_extra_q.\")\n",
    "    parser.add_argument(\"--repo\", type=Path,\n",
    "                        help=\"Folder containing all pre-trained models for use with -n.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ab4747",
   "metadata": {},
   "source": [
    "## 해석-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb594c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_model_flags에서 정한 옵션에 따라 모델을 불러올 수 있게 함수 생성\n",
    "# 로컬 모델 vs AWS에 있는 pretrained 모델 \n",
    "# repo 정보 추가\n",
    "def get_model_from_args(args):\n",
    "    \"\"\"\n",
    "    Load local model package or pre-trained model.\n",
    "    \"\"\"\n",
    "    return get_model(name=args.name, repo=args.repo)\n",
    "\n",
    "\n",
    "def get_model(name: str,\n",
    "              repo: tp.Optional[Path] = None):\n",
    "    \"\"\"`name` must be a bag of models name or a pretrained signature\n",
    "    from the remote AWS model repo or the specified local repo if `repo` is not None.\n",
    "    \"\"\"\n",
    "    if name == 'demucs_unittest':\n",
    "        return demucs_unittest()\n",
    "    model_repo: ModelOnlyRepo\n",
    "        \n",
    "    # repo를 지정하지 않는다면 AWS에서 pretrained 모델을 가져온다\n",
    "    if repo is None:\n",
    "        remote_files = [line.strip()\n",
    "                        for line in (REMOTE_ROOT / 'files.txt').read_text().split('\\n')\n",
    "                        if line.strip()]\n",
    "        model_repo = RemoteRepo(ROOT_URL, remote_files)\n",
    "        # 해석-3 셀 참조\n",
    "        # 요약 : 모델 4개 불러오기 + 모델 소스/채널/sr 같은지 검사\n",
    "        bag_repo = BagOnlyRepo(REMOTE_ROOT, model_repo)\n",
    "    \n",
    "    # 로컬 모델이어서 repo가 있다면 경로를 가져와서 모델 불러온다\n",
    "    else:\n",
    "        if not repo.is_dir():\n",
    "            fatal(f\"{repo} must exist and be a directory.\")\n",
    "        model_repo = LocalRepo(repo)\n",
    "        bag_repo = BagOnlyRepo(repo, model_repo)\n",
    "    any_repo = AnyModelRepo(model_repo, bag_repo)\n",
    "    return any_repo.get_model(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b3c981",
   "metadata": {},
   "source": [
    "# 해석-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e157819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOnlyRepo:\n",
    "    \"\"\"Handles only YAML files containing bag of models, leaving the actual\n",
    "    model loading to some Repo.\n",
    "    \"\"\"\n",
    "    def __init__(self, root: Path, model_repo: ModelOnlyRepo):\n",
    "        self.root = root\n",
    "        self.model_repo = model_repo\n",
    "        self.scan()\n",
    "    \n",
    "    # 아래 셀에 논리흐름 기재했음\n",
    "    def scan(self):\n",
    "        self._bags = {}\n",
    "        # root 폴더 경로에서 파일 명을 generator 형식(listdir와 다른점)으로 읽어 들인다\n",
    "        for file in self.root.iterdir():\n",
    "            if file.suffix == '.yaml':\n",
    "                # AWS 서버의 yaml 파일 안의 stem 옵션을 index로 준다\n",
    "                # 그 인덱스에 yaml 파일을 삽입한다\n",
    "                self._bags[file.stem] = file\n",
    "\n",
    "    def has_model(self, name: str) -> bool:\n",
    "        return name in self._bags\n",
    "\n",
    "    def get_model(self, name: str) -> BagOfModels:\n",
    "        try:\n",
    "            yaml_file = self._bags[name]\n",
    "        except KeyError:\n",
    "            raise ModelLoadingError(f'{name} is neither a single pre-trained model or '\n",
    "                                    'a bag of models.')\n",
    "        bag = yaml.safe_load(open(yaml_file))\n",
    "        signatures = bag['models']\n",
    "        # AWS는 모델 4개 사용\n",
    "        models = [self.model_repo.get_model(sig) for sig in signatures]\n",
    "        weights = bag.get('weights')\n",
    "        segment = bag.get('segment')\n",
    "        # 해석-4 셀 참조\n",
    "        return BagOfModels(models, weights, segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72f40e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('.DS_Store'),\n",
       " PosixPath('Frequency Positional Encoding - Test.ipynb'),\n",
       " PosixPath('corrupted excel file.ipynb'),\n",
       " PosixPath('Positional encoding.ipynb'),\n",
       " PosixPath('Separated 코드 분석.ipynb'),\n",
       " PosixPath('2021년 공문접수(맟춤).xlsx'),\n",
       " PosixPath('.ipynb_checkpoints'),\n",
       " PosixPath('aliasing test.ipynb')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path('.')\n",
    "list(path.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b4b4bd",
   "metadata": {},
   "source": [
    "# 해석-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af65a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfModels(nn.Module):\n",
    "    def __init__(self, models: tp.List[Model],\n",
    "                 weights: tp.Optional[tp.List[tp.List[float]]] = None,\n",
    "                 segment: tp.Optional[float] = None):\n",
    "        \"\"\"\n",
    "        Represents a bag of models with specific weights.\n",
    "        You should call `apply_model` rather than calling directly the forward here for\n",
    "        optimal performance.\n",
    "\n",
    "        Args:\n",
    "            models (list[nn.Module]): list of Demucs/HDemucs models.\n",
    "            weights (list[list[float]]): list of weights. If None, assumed to\n",
    "                be all ones, otherwise it should be a list of N list (N number of models),\n",
    "                each containing S floats (S number of sources).\n",
    "            segment (None or float): overrides the `segment` attribute of each model\n",
    "                (this is performed inplace, be careful is you reuse the models passed).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert len(models) > 0\n",
    "        first = models[0]\n",
    "        \n",
    "        # 핵심은 여러 개의 모델이 모두 같은\n",
    "        # 소스, sr, 채널을 같게끔 조정하는 것\n",
    "        for other in models:\n",
    "            assert other.sources == first.sources\n",
    "            assert other.samplerate == first.samplerate\n",
    "            assert other.audio_channels == first.audio_channels\n",
    "            if segment is not None:\n",
    "                other.segment = segment\n",
    "\n",
    "        self.audio_channels = first.audio_channels\n",
    "        self.samplerate = first.samplerate\n",
    "        self.sources = first.sources\n",
    "        self.models = nn.ModuleList(models)\n",
    "\n",
    "        if weights is None:\n",
    "            weights = [[1. for _ in first.sources] for _ in models]\n",
    "        else:\n",
    "            assert len(weights) == len(models)\n",
    "            for weight in weights:\n",
    "                assert len(weight) == len(first.sources)\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError(\"Call `apply_model` on this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c8f0e0",
   "metadata": {},
   "source": [
    "# 해석-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fdc804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_track(track, device, audio_channels, samplerate):\n",
    "    errors = {}\n",
    "    wav = None\n",
    "\n",
    "    try:\n",
    "        wav = AudioFile(track).read(\n",
    "            streams=0,\n",
    "            samplerate=samplerate,\n",
    "            channels=audio_channels).to(device)\n",
    "    except FileNotFoundError:\n",
    "        errors['ffmpeg'] = 'Ffmpeg is not installed.'\n",
    "    except subprocess.CalledProcessError:\n",
    "        errors['ffmpeg'] = 'FFmpeg could not read the file.'\n",
    "\n",
    "    if wav is None:\n",
    "        try:\n",
    "            # 오류 나도 한 번 더 기회줘라\n",
    "            # track이 str type이 아니어서 오류난 것일 수 있다\n",
    "            wav, sr = ta.load(str(track))\n",
    "        except RuntimeError as err:\n",
    "            errors['torchaudio'] = err.args[0]\n",
    "        else:\n",
    "            wav = wav.to(device)\n",
    "            wav = convert_audio(wav, sr, samplerate, audio_channels)\n",
    "    # 여러 경우를 고려해봤는데 이건 loading 에러다\n",
    "    if wav is None:\n",
    "        print(f\"Could not load file {track}. \"\n",
    "              \"Maybe it is not a supported file format? \")\n",
    "        for backend, error in errors.items():\n",
    "            print(f\"When trying to load using {backend}, got the following error: {error}\")\n",
    "        sys.exit(1)\n",
    "    return wav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b724fe68",
   "metadata": {},
   "source": [
    "# 해석-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586ae320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 실행 + command에서 progress bar 표시\n",
    "def apply_model(model, mix, shifts=1, split=True,\n",
    "                overlap=0.25, transition_power=1., progress=False):\n",
    "    \"\"\"\n",
    "    Apply model to a given mixture.\n",
    "\n",
    "    Args:\n",
    "        shifts (int): if > 0, will shift in time `mix` by a random amount between 0 and 0.5 sec\n",
    "            and apply the oppositve shift to the output. This is repeated `shifts` time and\n",
    "            all predictions are averaged. This effectively makes the model time equivariant\n",
    "            and improves SDR by up to 0.2 points.\n",
    "        split (bool): if True, the input will be broken down in 8 seconds extracts\n",
    "            and predictions will be performed individually on each and concatenated.\n",
    "            Useful for model with large memory footprint like Tasnet.\n",
    "        progress (bool): if True, show a progress bar (requires split=True)\n",
    "    \"\"\"\n",
    "    # 모델이 여러개일 경우 for loop를 통해 일일이 계산하도록 세팅\n",
    "    if isinstance(model, BagOfModels):\n",
    "        # Special treatment for bag of model.\n",
    "        # We explicitely apply multiple times `apply_model` so that the random shifts\n",
    "        # are different for each model.\n",
    "        estimates = 0\n",
    "        totals = [0] * len(model.sources)\n",
    "        for sub_model, weight in zip(model.models, model.weights):\n",
    "            out = apply_model(\n",
    "                sub_model, mix,\n",
    "                shifts=shifts, split=split, overlap=overlap,\n",
    "                transition_power=transition_power, progress=progress)\n",
    "            # 이전 단계의 가중치로 다음 단계 out 계산\n",
    "            for k in range(out.shape[0]):\n",
    "                out[k] *= weight[k]\n",
    "                totals[k] += weight[k]\n",
    "            estimates += out\n",
    "        for k in range(estimates.shape[0]):\n",
    "            estimates[k] /= totals[k]\n",
    "        return estimates\n",
    "\n",
    "    assert transition_power >= 1, \"transition_power < 1 leads to weird behavior.\"\n",
    "    device = mix.device\n",
    "    batch, channels, length = mix.shape\n",
    "    \n",
    "    # wave 8초 단위로 잘라서 계산 진행\n",
    "    if split:\n",
    "        out = th.zeros(batch, len(model.sources), channels, length, device=device)\n",
    "        sum_weight = th.zeros(length, device=device)\n",
    "        segment = int(model.samplerate * model.segment)\n",
    "        stride = int((1 - overlap) * segment)\n",
    "        offsets = range(0, length, stride)\n",
    "        scale = stride / model.samplerate\n",
    "        if progress:\n",
    "            offsets = tqdm.tqdm(offsets, unit_scale=scale, ncols=120, unit='seconds')\n",
    "        # We start from a triangle shaped weight, with maximal weight in the middle\n",
    "        # of the segment. Then we normalize and take to the power `transition_power`.\n",
    "        # Large values of transition power will lead to sharper transitions.\n",
    "        \n",
    "        # 가중치 결합\n",
    "        weight = th.cat([th.arange(1, segment // 2 + 1),\n",
    "                         th.arange(segment - segment // 2, 0, -1)]).to(device)\n",
    "        assert len(weight) == segment\n",
    "        # If the overlap < 50%, this will translate to linear transition when\n",
    "        # transition_power is 1.\n",
    "        weight = (weight / weight.max())**transition_power\n",
    "        for offset in offsets:\n",
    "            chunk = TensorChunk(mix, offset, segment)\n",
    "            chunk_out = apply_model(model, chunk, shifts=shifts, split=False)\n",
    "            chunk_length = chunk_out.shape[-1]\n",
    "            out[..., offset:offset + segment] += weight[:chunk_length] * chunk_out\n",
    "            sum_weight[offset:offset + segment] += weight[:chunk_length]\n",
    "            offset += segment\n",
    "        assert sum_weight.min() > 0\n",
    "        out /= sum_weight\n",
    "        return out\n",
    "    \n",
    "    # frequency 관련 추가 설정임\n",
    "    # default는 0.1\n",
    "    elif shifts:\n",
    "        max_shift = int(0.5 * model.samplerate)\n",
    "        mix = tensor_chunk(mix)\n",
    "        padded_mix = mix.padded(length + 2 * max_shift)\n",
    "        out = 0\n",
    "        for _ in range(shifts):\n",
    "            offset = random.randint(0, max_shift)\n",
    "            shifted = TensorChunk(padded_mix, offset, length + max_shift - offset)\n",
    "            shifted_out = apply_model(model, shifted, shifts=0, split=False)\n",
    "            out += shifted_out[..., max_shift - offset:]\n",
    "        out /= shifts\n",
    "        return out\n",
    "    else:\n",
    "        if hasattr(model, 'valid_length'):\n",
    "            valid_length = model.valid_length(length)\n",
    "        else:\n",
    "            valid_length = length\n",
    "        mix = tensor_chunk(mix)\n",
    "        padded_mix = mix.padded(valid_length)\n",
    "        with th.no_grad():\n",
    "            out = model(padded_mix)\n",
    "        return center_trim(out, length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
