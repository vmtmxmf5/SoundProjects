{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83c4996c",
   "metadata": {},
   "source": [
    "## Embedding Output Shape?\n",
    "\n",
    ": (Batch, Sample, Token(=Time Steps), d_model(=emb_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c72e625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(B, S, T) : torch.Size([2, 3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1, 17, 18, 19],\n",
       "         [ 2, 22, 23, 24],\n",
       "         [ 3,  4,  5,  6]],\n",
       "\n",
       "        [[ 1, 13, 15, 17],\n",
       "         [ 2, 21, 28, 24],\n",
       "         [ 4,  5, 13, 15]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[1, 17, 18, 19],\n",
    "                [2, 22, 23, 24],\n",
    "                 [3, 4, 5, 6]])\n",
    "b = torch.tensor([[1, 13, 15, 17],\n",
    "                 [2, 21, 28, 24],\n",
    "                 [4, 5, 13, 15]])\n",
    "c = torch.stack([a, b])\n",
    "print('(B, S, T) :', c.shape)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f547977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Batch, Sample, Token, d_model) :  torch.Size([2, 3, 4, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.2584,  1.8426,  1.0916,  0.9989, -0.5735, -0.3674, -0.9559],\n",
       "          [ 0.2777,  0.2957,  1.1920, -0.5935, -0.4059, -1.0595, -2.1664],\n",
       "          [-0.8182, -1.4669,  0.7383, -0.7970,  0.2455, -1.2437, -2.1387],\n",
       "          [ 0.1167, -0.7758, -0.3058, -0.7731,  0.6431,  1.0676, -0.7772]],\n",
       "\n",
       "         [[-0.1114, -0.0111, -0.0706,  1.0348, -1.7673,  0.7506,  0.4907],\n",
       "          [-0.3289, -0.1181, -0.3083,  0.5114,  1.3850,  1.2151, -1.9012],\n",
       "          [-0.4969,  1.4924, -1.2457, -0.6173, -0.3539, -0.1250,  1.2120],\n",
       "          [-1.1135, -0.9453,  0.7297,  1.8926,  0.3260, -1.3908,  0.1047]],\n",
       "\n",
       "         [[ 0.8959,  0.7753, -0.4108, -0.5333,  0.5469,  1.4377, -0.0402],\n",
       "          [ 1.5575, -1.5602,  0.1295,  0.0712,  0.8427,  1.0777, -0.0539],\n",
       "          [-0.4557, -0.5587, -0.4264, -2.0967, -0.0101,  1.4386, -0.7226],\n",
       "          [ 0.3589, -1.3021,  0.1996, -1.5563, -1.3508,  0.6137,  1.9654]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2584,  1.8426,  1.0916,  0.9989, -0.5735, -0.3674, -0.9559],\n",
       "          [ 0.5545,  0.8878, -1.2181,  1.1163,  1.0141,  0.6639,  0.9381],\n",
       "          [-0.5820,  0.1470, -1.5467,  0.9842, -0.8520, -0.3226,  0.2129],\n",
       "          [ 0.2777,  0.2957,  1.1920, -0.5935, -0.4059, -1.0595, -2.1664]],\n",
       "\n",
       "         [[-0.1114, -0.0111, -0.0706,  1.0348, -1.7673,  0.7506,  0.4907],\n",
       "          [-1.3877,  0.4537, -1.1874, -1.6989, -0.3649, -1.8602,  0.0830],\n",
       "          [-0.8654,  1.2634,  0.0121,  0.3525,  1.3323,  1.4939, -1.3556],\n",
       "          [-1.1135, -0.9453,  0.7297,  1.8926,  0.3260, -1.3908,  0.1047]],\n",
       "\n",
       "         [[ 1.5575, -1.5602,  0.1295,  0.0712,  0.8427,  1.0777, -0.0539],\n",
       "          [-0.4557, -0.5587, -0.4264, -2.0967, -0.0101,  1.4386, -0.7226],\n",
       "          [ 0.5545,  0.8878, -1.2181,  1.1163,  1.0141,  0.6639,  0.9381],\n",
       "          [-0.5820,  0.1470, -1.5467,  0.9842, -0.8520, -0.3226,  0.2129]]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Batch, d_model\n",
    "emb = nn.Embedding(29, 7)\n",
    "print('(Batch, Sample, Token, d_model) : ', emb(c).shape)\n",
    "# 참고로 len(vocab)이 input으로 들어오는 id+1 범위여야 작동한다\n",
    "emb(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3342b560",
   "metadata": {},
   "source": [
    "## Scaled Embedding\n",
    "\n",
    "깃헙 코드 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "700ce340",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Boost learning rate for embeddings (with `scale`).\n",
    "    Also, can make embeddings continuous with `smooth`.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int,\n",
    "                 scale: float = 10., smooth=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        if smooth:\n",
    "            weight = torch.cumsum(self.embedding.weight.data, dim=0)\n",
    "            # when summing gaussian, overscale raises as sqrt(n), so we nornalize by that.\n",
    "            weight = weight / torch.arange(1, num_embeddings + 1).to(weight).sqrt()[:, None]\n",
    "            self.embedding.weight.data[:] = weight\n",
    "        self.embedding.weight.data /= scale\n",
    "        self.scale = scale\n",
    "\n",
    "    @property\n",
    "    def weight(self):\n",
    "        return self.embedding.weight * self.scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x) * self.scale\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "378d044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = 4096 // 2\n",
    "chin_z = 2\n",
    "emb_smooth = True\n",
    "emb_scale = 10\n",
    "freq_embs = 0.2\n",
    "\n",
    "# 주파수 2048까지만 사용하니까, len(vocab)이 2048이 된 것\n",
    "freq_emb = ScaledEmbedding(freqs, chin_z, smooth=emb_smooth, scale=emb_scale)\n",
    "freq_emb_scale = freq_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8fb48982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ScaledEmbedding(\n",
       "  (embedding): Embedding(2048, 2)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2ec7fb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B, C, F, T\n",
    "x = torch.randn((3, 2, 150, 200))\n",
    "\n",
    "if freq_emb is not None:\n",
    "    # x.shape[-2] == freq\n",
    "    # 주파수 인덱스\n",
    "    frs = torch.arange(x.shape[-2], device=x.device)\n",
    "    \n",
    "    # Transpose + 차원추가 + X와 같게 B와 Time steps 추가(좌우 복사)\n",
    "    emb = freq_emb(frs).t()[None, :, :, None].expand_as(x)\n",
    "    x = x + freq_emb_scale * emb\n",
    "##############################################################\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7b67877a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 150, 200])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38db1e7",
   "metadata": {},
   "source": [
    "### 핵심 : 참조논문 그대로 코드를 구현하지 않았다 (=참조 논문 수식과 다름. cos 존재 x)\n",
    "실험 결과, smoothing을 통해  \n",
    "**인접한 freq.들이 처음에 유사한 값**을 가지도록 만든 뒤,  \n",
    "학습을 통해 **pos emb들을 학습**하도록 코드를 구현했다  \n",
    "> 주파수 축에 Conv.을 사용하는 것은 픽셀에 Conv. 사용하는 것과 다르게 그 의미가 다르다  \n",
    "전문적인 용어로 말하자면 이동에 대한 국소적 불변성(invariant to translation)이 성립하지 않기 때문이고,  \n",
    "직관적으로 설명하자면 이미지 픽셀과 다르게 주파수는 **범위에 높고 낮음에 따라 그 의미**가 달라지기 때문이다    \n",
    "예컨대, 주파수 범위에 따라 굵은 남자 목소리인지, 높은 여성 목소리인지가 달라지듯 말이다  \n",
    "따라서 주파수에 순서 정보를 추가해야 한다\n",
    "\n",
    "\n",
    "in Hybrid Demucs Github  \n",
    "\"add frequency embedding **to allow for non equivariant convolutions** over the frequency axis.\"\n",
    "\n",
    "---\n",
    "1. smoothing 실험  \n",
    "2. expand_as 실험 (차원 조정)\n",
    "---\n",
    "\n",
    "in Hybrid Demucs Paper...  \n",
    "\"To account for that, Isik et al. (2020) suggest injecting an embedding of the frequency before\n",
    "applying the convolution. We use the same approach, with the addition that we smooth the\n",
    "initial embedding so that close frequencies have similar embeddings.\"  \n",
    "  \n",
    "https://arxiv.org/pdf/2008.04470.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8c613634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.2759,  0.8243],\n",
      "        [ 1.3810, -1.8629],\n",
      "        [ 0.0308, -1.1921],\n",
      "        [ 0.1048, -0.3961],\n",
      "        [ 0.5584,  0.1597]], requires_grad=True)\n",
      "tensor([[-0.2759,  0.8243],\n",
      "        [ 1.1051, -1.0385],\n",
      "        [ 1.1359, -2.2307],\n",
      "        [ 1.2408, -2.6267],\n",
      "        [ 1.7992, -2.4671]])\n",
      "sqrt idx :  tensor([[1.0000],\n",
      "        [1.4142],\n",
      "        [1.7321],\n",
      "        [2.0000],\n",
      "        [2.2361]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2759,  0.8243],\n",
       "        [ 0.7815, -0.7344],\n",
       "        [ 0.6558, -1.2879],\n",
       "        [ 0.6204, -1.3134],\n",
       "        [ 0.8046, -1.1033]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# smoothing\n",
    "# 인접한 freq.들이 처음에 유사한 값을 가지도록 조정\n",
    "tmp = nn.Embedding(5, 2)\n",
    "print(tmp.weight)\n",
    "print(torch.cumsum(tmp.weight.data, dim=0))\n",
    "\n",
    "weight = torch.cumsum(tmp.weight.data, dim=0)\n",
    "weight = weight / torch.arange(1, 5+1).to(weight).sqrt()[:, None]\n",
    "print('sqrt idx : ',  torch.arange(1, 5+1).to(weight).sqrt()[:, None])\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d60d3f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 5, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.2759, -0.2759, -0.2759],\n",
       "          [ 0.7815,  0.7815,  0.7815],\n",
       "          [ 0.6558,  0.6558,  0.6558],\n",
       "          [ 0.6204,  0.6204,  0.6204],\n",
       "          [ 0.8046,  0.8046,  0.8046]],\n",
       "\n",
       "         [[ 0.8243,  0.8243,  0.8243],\n",
       "          [-0.7344, -0.7344, -0.7344],\n",
       "          [-1.2879, -1.2879, -1.2879],\n",
       "          [-1.3134, -1.3134, -1.3134],\n",
       "          [-1.1033, -1.1033, -1.1033]]]], grad_fn=<ExpandBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# expand test\n",
    "tmp.weight = nn.Parameter(weight)\n",
    "res = tmp(torch.arange(0, 5)).t()[None, :, :, None]\n",
    "ex = torch.randn((1,2,5,3))\n",
    "res = res.expand_as(ex)\n",
    "print(res.shape)\n",
    "res # B, C, F, T\n",
    "# column과 마지막 row는? 바로 토큰 pos다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dd56e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
